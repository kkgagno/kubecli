import os
print("Executing app.py")
import yaml
import datetime
import pyhelm3 as helm
import json
import pytz
import re
import subprocess
import threading # Import threading
import time
import uuid # Import uuid
import zipfile
from flask import Flask, render_template, Response, redirect, url_for, request, flash, session, send_from_directory, jsonify, make_response
import pdfkit
from bs4 import BeautifulSoup
import csv
from io import StringIO
from werkzeug.security import generate_password_hash, check_password_hash
import markdown
from functools import wraps
from kubernetes import client, config
import paramiko
import socket
import ollama
from cleanup import cleanup_old_files



app = Flask(__name__)
app.secret_key = '3727fc9d59984122d856c7faa4b9078cf2ec7a74b857f62c' # Required for flash messages
app.config['SESSION_COOKIE_NAME'] = 'simple_kube_session'

@app.context_processor
def inject_now():
    return {'now': datetime.datetime.utcnow}

# Dictionary to store the status of long-running tasks
running_tasks = {}

# --- Configuration Variables ---
KUBECONFIG_PATH = os.environ.get("KUBECONFIG_PATH", os.path.expanduser("~/.kube/config"))
SSH_KEY_PATH = os.environ.get("SSH_KEY_PATH", os.path.expanduser("~/.ssh/id_rsa"))
SSH_USERNAME = os.environ.get("SSH_USERNAME", "keith")
HELM_EXECUTABLE_PATH = os.environ.get("HELM_EXECUTABLE_PATH", "helm")
# -----------------------------

# Load Kubernetes configuration
try:
    config.load_kube_config(config_file=KUBECONFIG_PATH)
except config.ConfigException:
    # If running inside a cluster, use in-cluster config
    try:
        config.load_incluster_config()
    except config.ConfigException:
        raise Exception("Could not configure kubernetes client")

core_api = client.CoreV1Api()
apps_api = client.AppsV1Api()
batch_api = client.BatchV1Api()

def login_required(f):
    @wraps(f)
    def decorated_function(*args, **kwargs):
        app.logger.info(f"Session object: {session}")
        if 'logged_in' not in session:
            return redirect(url_for('login'))
        return f(*args, **kwargs)
    return decorated_function

import logging

logging.basicConfig(filename='app.log', level=logging.INFO)

def generate_detailed_oscap_csv():
    xml_dir = os.path.join(os.path.dirname(__file__), 'tmp_reports')
    reports_dir = os.path.join(app.static_folder, 'oscap_reports')
    
    if not os.path.exists(reports_dir):
        os.makedirs(reports_dir)

    # Use a fixed name for the main CSV and a timestamped name for the archive
    main_csv_filename = "oscap_detailed_summary.csv"
    main_csv_path = os.path.join(reports_dir, main_csv_filename)
    
    timestamp = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')
    archive_csv_filename = f"{timestamp}_oscap_detailed_summary.csv"
    archive_csv_path = os.path.join(reports_dir, archive_csv_filename)

    summary_data = [['Hostname', 'Rule ID', 'Severity', 'Result']]

    if os.path.exists(xml_dir):
        for filename in os.listdir(xml_dir):
            if not filename.endswith('.xml'):
                continue

            hostname = filename.replace('scan-results-', '').replace('.xml', '')
            xml_path = os.path.join(xml_dir, filename)

            with open(xml_path, 'r') as f:
                soup = BeautifulSoup(f, 'xml')

            all_rules = soup.find_all('Rule')
            results = soup.find_all('rule-result')
            
            rules_info = {rule['id']: rule['severity'] for rule in all_rules if 'severity' in rule.attrs}

            for result in results:
                rule_id = result['idref']
                severity = rules_info.get(rule_id, 'N/A')
                res = result.find('result').text
                summary_data.append([hostname, rule_id, severity, res])

    # Write to the main, fixed-name CSV
    with open(main_csv_path, 'w', newline='') as f:
        writer = csv.writer(f)
        writer.writerows(summary_data)
        
    # Also write to the timestamped archive CSV
    with open(archive_csv_path, 'w', newline='') as f:
        writer = csv.writer(f)
        writer.writerows(summary_data)

def run_ansible_playbook_async(task_id, task_type, playbook_path, inventory_path, extra_vars=None, limit=None, chained_task=False):
    if not chained_task:
        running_tasks[task_id] = {'status': 'running', 'output': '', 'type': task_type}
    
    command = [
        os.path.join(os.path.dirname(__file__), 'venv/bin/ansible-playbook'),
        '-i', inventory_path,
        '--user', SSH_USERNAME,
        '--private-key', SSH_KEY_PATH,
        '--extra-vars', 'ansible_ssh_common_args="-o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null"',
        '--extra-vars', 'ansible_python_interpreter=/usr/bin/python3'
    ]
    if limit:
        command.extend(['--limit', f"{limit},localhost"])
    if extra_vars:
        for key, value in extra_vars.items():
            command.extend(['--extra-vars', f'{key}={value}'])
    command.append(playbook_path)

    log_file_path = os.path.join(os.path.dirname(__file__), "ansible_command.log")

    def worker():
        with open(log_file_path, "w") as f:
            f.write(f"Executing Ansible command: {' '.join(command)}\n\n")
        
        try:
            process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True, bufsize=1, universal_newlines=True)
            
            for line in iter(process.stdout.readline, ''):
                running_tasks[task_id]['output'] += line
                with open(log_file_path, "a") as f:
                    f.write(line)
            
            process.stdout.close()
            return_code = process.wait()

            if return_code != 0:
                running_tasks[task_id]['status'] = 'failed'
                error_message = f"\nPlaybook failed with exit code {return_code}."
                running_tasks[task_id]['output'] += error_message
                with open(log_file_path, "a") as f:
                    f.write(error_message)

        except Exception as e:
            error_message = f"An error occurred: {e}"
            running_tasks[task_id]['status'] = 'error'
            running_tasks[task_id]['output'] = error_message
            with open(log_file_path, "a") as f:
                f.write(error_message)

    thread = threading.Thread(target=worker)
    thread.start()
    return thread

@app.route('/run_oscap_scan', methods=['POST'])
@login_required
def run_oscap_scan():
    for task_id, task_info in running_tasks.items():
        if task_info.get('type') == 'oscap' and task_info.get('status') == 'running':
            hosts = task_info.get('hosts', [])
            return jsonify({'message': f'OpenSCAP scan is already running on the following hosts: {", ".join(hosts)}'}), 409

    selected_hosts = request.form.getlist('selected_hosts')
    if not selected_hosts:
        return jsonify({'message': 'No hosts selected for the scan.'}), 400

    limit = ",".join(selected_hosts)
    task_id = str(uuid.uuid4())
    
    inventory_path = os.path.join(os.path.dirname(__file__), 'inventory.ini')
    scan_playbook_path = os.path.join(os.path.dirname(__file__), 'ansible_oscap_scan', 'oscap_scan.yml')
    report_playbook_path = os.path.join(os.path.dirname(__file__), 'ansible_oscap_scan', 'generate_reports.yml')

    def worker():
        # Start the main task
        running_tasks[task_id] = {'status': 'running', 'output': 'Starting OpenSCAP scan...\n', 'type': 'oscap', 'hosts': selected_hosts}
        
        # Run the scan playbook
        scan_thread = run_ansible_playbook_async(task_id, 'oscap', scan_playbook_path, inventory_path, None, limit, chained_task=True)
        scan_thread.join() # Wait for the scan to complete
        
        # Check if the scan was successful
        if running_tasks.get(task_id, {}).get('status') == 'failed':
            return # Stop if the scan failed

        running_tasks[task_id]['output'] += '\nScan complete. Generating reports...\n'

        # Run the report generation playbook
        report_thread = run_ansible_playbook_async(task_id, 'oscap', report_playbook_path, inventory_path, None, None, chained_task=True)
        report_thread.join() # Wait for report generation to complete

        # Check if report generation was successful
        if running_tasks.get(task_id, {}).get('status') == 'failed':
            return # Stop if report generation failed

        # If all steps are successful, mark as completed_and_reloaded
        running_tasks[task_id]['status'] = 'completed_and_reloaded'
        generate_detailed_oscap_csv()

    thread = threading.Thread(target=worker)
    thread.start()
    flash('OpenSCAP scan and report generation started.', 'info')
    return redirect(url_for('compliance_reports'))


import socket

def get_inventory_hosts():
    inventory_path = os.path.join(os.path.dirname(__file__), 'inventory.ini')
    hosts = set()
    try:
        with open(inventory_path, 'r') as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith('#') and not line.startswith('['):
                    hostname = line.split(' ')[0]
                    hosts.add(hostname)
        return sorted(list(hosts))
    except FileNotFoundError:
        return []


@app.route('/run_report_playbook', methods=['POST'])
@login_required
def run_report_playbook():
    for task_id, task_info in running_tasks.items():
        if task_info.get('type') == 'report' and task_info.get('status') == 'running':
            return jsonify({'message': 'Compliance report generation is already running.'}), 409

    task_id = str(uuid.uuid4())
    playbook_path = os.path.join(os.path.dirname(__file__), 'ansible_goss_compliance', 'report_playbook.yml')
    
    inventory_path = f"/tmp/report_inventory_{task_id}.ini"
    with open(inventory_path, "w") as f:
        f.write("localhost ansible_connection=local\n")

    # Move old reports to archive
    reports_dir = os.path.join(app.static_folder, 'goss_reports')
    archive_dir = os.path.join(app.static_folder, 'goss_reports_archive')
    now = datetime.datetime.now()
    
    if not os.path.exists(archive_dir):
        os.makedirs(archive_dir)

    for filename in os.listdir(reports_dir):
        if filename.endswith('.html'):
            report_path = os.path.join(reports_dir, filename)
            archive_path = os.path.join(archive_dir, f"{now.strftime('%Y-%m-%d_%H-%M-%S')}_{filename}")
            os.rename(report_path, archive_path)

    # Enforce retention policy
    for filename in os.listdir(archive_dir):
        archive_path = os.path.join(archive_dir, filename)
        try:
            file_time_str = filename.split('_')[0]
            file_time = datetime.datetime.strptime(file_time_str, '%Y-%m-%d')
            if (now - file_time).days > 10:
                os.remove(archive_path)
        except (ValueError, IndexError):
            # Handle files that don't match the expected naming convention
            pass

    thread = threading.Thread(target=run_ansible_playbook_async, args=(task_id, 'report', playbook_path, inventory_path, {'goss_report_dir': os.path.join(app.static_folder, 'goss_reports')}))
    thread.start()
    return jsonify({'task_id': task_id, 'message': 'Compliance report generation started.'}), 202


@app.route('/get_task_status/<task_id>')
@login_required
def get_task_status(task_id):
    task = running_tasks.get(task_id)
    if task:
        return jsonify(task)
    return jsonify({'status': 'not_found', 'message': 'Task not found.'}), 404

@app.route('/get_all_task_statuses')
@login_required
def get_all_task_statuses():
    return jsonify(running_tasks)

@app.route('/playbook_output/<task_id>')
@login_required
def playbook_output(task_id):
    task = running_tasks.get(task_id)
    if task:
        return render_template('playbook_output.html', task=task)
    return "Task not found or output not available.", 404

@app.route('/readme')
@login_required
def readme():
    with open('README.md', 'r') as f:
        content = f.read()
    html_content = markdown.markdown(content)
    return render_template('readme.html', content=html_content)

@app.route('/login', methods=['GET', 'POST'])

def login():
    if request.method == 'POST':
        username = request.form['username']
        entered_password = request.form['password']
        
        stored_password_hash = None
        try:
            with open('password.txt', 'r') as f:
                stored_password_hash = f.read().strip()
        except FileNotFoundError:
            pass  # No password file, assume default blank password

        if username == 'admin':
            if stored_password_hash:
                # Password file exists, check against hash
                if check_password_hash(stored_password_hash, entered_password):
                    session['logged_in'] = True
                    session.permanent = False
                    return redirect(url_for('index'))
                else:
                    flash('Invalid credentials', 'error')
            else:
                # No password file, allow blank password
                if entered_password == '':
                    session['logged_in'] = True
                    session.permanent = False
                    return redirect(url_for('index'))
                else:
                    flash('Invalid credentials', 'error')
        else:
            flash('Invalid credentials', 'error')
    return render_template('login.html')


@app.route('/change_password', methods=['GET', 'POST'])
@login_required
def change_password():
    if request.method == 'POST':
        new_password = request.form['new_password']
        confirm_password = request.form['confirm_password']
        if new_password == confirm_password:
            with open('password.txt', 'w') as f:
                f.write(generate_password_hash(new_password))
            flash('Password changed successfully!', 'success')
            return redirect(url_for('index'))
        else:
            flash('Passwords do not match.', 'error')
    return render_template('change_password.html')


@app.route('/logout')
def logout():
    session.pop('logged_in', None)
    return redirect(url_for('login'))

@app.route('/')
@login_required
def index():
    return redirect(url_for('get_pods'))

@app.route('/pods')
@login_required
def get_pods():
    search_query = request.args.get('search')
    selected_node_name = request.args.get('node_name', 'all') # Default to 'all'
    selected_namespace = request.args.get('namespace', 'all') # Default to 'all'

    try:
        # Fetch all namespaces for the dropdown
        namespaces_obj = core_api.list_namespace(watch=False)
        namespaces = sorted([ns.metadata.name for ns in namespaces_obj.items])
        namespaces.insert(0, 'all') # Add 'all' option at the beginning

        # Fetch all node names for the dropdown
        nodes_obj = core_api.list_node(watch=False)
        nodes_list = sorted([node.metadata.name for node in nodes_obj.items])

        if selected_namespace == 'all':
            pods = core_api.list_pod_for_all_namespaces(watch=False)
        else:
            pods = core_api.list_namespaced_pod(namespace=selected_namespace, watch=False)

        filtered_pods = []
        for pod in pods.items:
            # Apply search query filter
            if search_query and search_query.lower() not in pod.metadata.name.lower():
                continue
            # Apply node name filter
            if selected_node_name != 'all' and pod.spec.node_name != selected_node_name:
                continue
            filtered_pods.append(pod)

        # Calculate age for each pod
        for pod in filtered_pods:
            pod.age = calculate_age(pod.metadata.creation_timestamp)

        return render_template('pods.html', pods=filtered_pods, search_query=search_query, nodes_list=nodes_list, selected_node_name=selected_node_name, namespaces=namespaces, selected_namespace=selected_namespace)
    except Exception as e:
        flash(f"Error fetching pods: {e}", 'error')
        return render_template('pods.html', pods=[], search_query=search_query, nodes_list=[], selected_node_name='all', namespaces=['all'], selected_namespace='all')

def calculate_age(creation_timestamp):
    now = datetime.datetime.now(pytz.utc)
    age = now - creation_timestamp
    
    days = age.days
    seconds = age.seconds
    
    hours = seconds // 3600
    minutes = (seconds % 3600) // 60
    
    if days > 0:
        return f"{days}d {hours}h"
    elif hours > 0:
        return f"{hours}h {minutes}m"
    elif minutes > 0:
        return f"{minutes}m"
    else:
        return "0m"

@app.route('/logs/<namespace>/<pod_name>')
@login_required
def pod_logs(namespace, pod_name):
    if 'logged_in' not in session:
        return "Unauthorized", 401
    container_name = request.args.get('container')
    try:
        # Determine the container name if not specified
        if not container_name:
            pod = core_api.read_namespaced_pod(name=pod_name, namespace=namespace)
            if pod.spec.containers:
                container_name = pod.spec.containers[0].name
            else:
                return "No containers found in the pod.", 404

        # Fetch logs
        logs = core_api.read_namespaced_pod_log(name=pod_name, namespace=namespace, container=container_name)
        
        # Check if logs are empty
        if not logs or not logs.strip():
            logs = "No logs available for this container."

        response = make_response(logs)
        response.headers['Content-Type'] = 'text/plain'
        return response
        
    except client.ApiException as e:
        try:
            # Try to parse the error body for a more specific message
            error_body = json.loads(e.body)
            error_message = error_body.get('message', str(e))
        except (json.JSONDecodeError, AttributeError):
            error_message = str(e)
        
        # Provide a clear error message to the frontend
        final_message = f"Error fetching logs for pod '{pod_name}' (container: '{container_name}'): {error_message}"
        return final_message, 500
        
    except Exception as e:
        app.logger.error(f"An unexpected error occurred in pod_logs: {e}", exc_info=True)
        return f"An unexpected error occurred: {e}", 500

@app.route('/delete_pod/<namespace>/<pod_name>', methods=['POST'])
@login_required
def delete_pod(namespace, pod_name):
    try:
        core_api.delete_namespaced_pod(name=pod_name, namespace=namespace)
        flash(f"Pod {pod_name} in namespace {namespace} deleted successfully.", 'success')
    except Exception as e:
        flash(f"Error deleting pod {pod_name} in namespace {namespace}: {e}", 'error')
    return redirect(url_for('get_pods'))

@app.route('/describe/<resource_type>/<namespace>/<name>')
@login_required
def describe_resource(resource_type, namespace, name):
    if 'logged_in' not in session:
        return "Unauthorized", 401
    try:
        if resource_type == 'pod':
            resource = core_api.read_namespaced_pod(name=name, namespace=namespace, _preload_content=False)
        elif resource_type == 'deployment':
            resource = apps_api.read_namespaced_deployment(name=name, namespace=namespace, _preload_content=False)
        elif resource_type == 'service':
            resource = core_api.read_namespaced_service(name=name, namespace=namespace, _preload_content=False)
        elif resource_type == 'node':
            resource = core_api.read_node(name=name, _preload_content=False)
        elif resource_type == 'persistentvolumeclaim':
            resource = core_api.read_namespaced_persistent_volume_claim(name=name, namespace=namespace, _preload_content=False)
        elif resource_type == 'configmap':
            resource = core_api.read_namespaced_config_map(name=name, namespace=namespace, _preload_content=False)
        elif resource_type == 'statefulset':
            resource = apps_api.read_namespaced_stateful_set(name=name, namespace=namespace, _preload_content=False)
        elif resource_type == 'daemonset':
            resource = apps_api.read_namespaced_daemon_set(name=name, namespace=namespace, _preload_content=False)
        elif resource_type == 'job':
            resource = batch_api.read_namespaced_job(name=name, namespace=namespace, _preload_content=False)
        elif resource_type == 'secret':
            resource = core_api.read_namespaced_secret(name=name, namespace=namespace, _preload_content=False)
        else:
            return f"Unknown resource type: {resource_type}", 400

        yaml_data = yaml.dump(yaml.safe_load(resource.data))
        response = make_response(yaml_data)
        response.headers['Content-Type'] = 'text/plain'
        return response

    except Exception as e:
        return f"Error describing resource: {e}", 500

@app.route('/deployments')
@login_required
def get_deployments():
    search_query = request.args.get('search')
    try:
        deployments = apps_api.list_deployment_for_all_namespaces(watch=False)
        if search_query:
            deployments.items = [dep for dep in deployments.items if search_query.lower() in dep.metadata.name.lower()]
        return render_template('deployments.html', deployments=deployments.items, search_query=search_query)
    except Exception as e:
        flash(f"Error fetching deployments: {e}", 'error')
        return render_template('deployments.html', deployments=[], search_query=search_query)

@app.route('/rollout_restart_deployment/<namespace>/<name>', methods=['POST'])
@login_required
def rollout_restart_deployment(namespace, name):
    try:
        now = datetime.datetime.utcnow().isoformat() + "Z"
        body = {
            "spec": {
                "template": {
                    "metadata": {
                        "annotations": {
                            "kubectl.kubernetes.io/restartedAt": now
                        }
                    }
                }
            }
        }
        apps_api.patch_namespaced_deployment(name=name, namespace=namespace, body=body)
        flash(f"Deployment {name} in namespace {namespace} restarted successfully.", 'success')
    except Exception as e:
        flash(f"Error restarting deployment {name} in namespace {namespace}: {e}", 'error')
    return redirect(url_for('get_deployments'))

@app.route('/delete_deployment/<namespace>/<name>', methods=['POST'])
@login_required
def delete_deployment(namespace, name):
    try:
        apps_api.delete_namespaced_deployment(name=name, namespace=namespace)
        flash(f"Deployment {name} in namespace {namespace} deleted successfully.", 'success')
    except Exception as e:
        flash(f"Error deleting deployment {name} in namespace {namespace}: {e}", 'error')
    return redirect(url_for('get_deployments'))


@app.route('/delete_service/<namespace>/<name>', methods=['POST'])
@login_required
def delete_service(namespace, name):
    try:
        core_api.delete_namespaced_service(name=name, namespace=namespace)
        flash(f"Service {name} in namespace {namespace} deleted successfully.", 'success')
    except Exception as e:
        flash(f"Error deleting service {name} in namespace {namespace}: {e}", 'error')
    return redirect(url_for('get_services'))


@app.route('/services')
@login_required
def get_services():
    search_query = request.args.get('search')
    try:
        services = core_api.list_service_for_all_namespaces(watch=False)
        if search_query:
            services.items = [svc for svc in services.items if search_query.lower() in svc.metadata.name.lower()]
        return render_template('services.html', services=services.items, search_query=search_query)
    except Exception as e:
        flash(f"Error fetching services: {e}", 'error')
        return render_template('services.html', services=[], search_query=search_query)

@app.route('/cordon_node/<node_name>', methods=['POST'])
@login_required
def cordon_node(node_name):
    try:
        body = {
            "spec": {
                "unschedulable": True
            }
        }
        core_api.patch_node(name=node_name, body=body)
        
        # Evict pods from the node
        pods = core_api.list_pod_for_all_namespaces(field_selector=f'spec.nodeName=={node_name}')
        evicted_pods = []
        ignored_pods = []
        errors = []
        for pod in pods.items:
            if pod.metadata.owner_references and any(
                owner.kind == "DaemonSet" for owner in pod.metadata.owner_references
            ):
                ignored_pods.append(f"{pod.metadata.namespace}/{pod.metadata.name}")
                continue

            eviction_body = client.V1Eviction(
                metadata=client.V1ObjectMeta(
                    name=pod.metadata.name,
                    namespace=pod.metadata.namespace
                )
            )
            try:
                core_api.create_namespaced_pod_eviction(
                    name=pod.metadata.name,
                    namespace=pod.metadata.namespace,
                    body=eviction_body
                )
                evicted_pods.append(f"{pod.metadata.namespace}/{pod.metadata.name}")
            except client.ApiException as e:
                errors.append(f"Error evicting pod {pod.metadata.name}: {e.reason}")
        
        if errors:
            return jsonify({'status': 'error', 'message': f'Node {node_name} cordoned, but failed to drain all pods.', 'errors': errors, 'evicted_pods': evicted_pods, 'ignored_pods': ignored_pods}), 500
        return jsonify({'status': 'success', 'message': f'Node {node_name} cordoned and drained successfully.', 'evicted_pods': evicted_pods, 'ignored_pods': ignored_pods})
    except Exception as e:
        return jsonify({'status': 'error', 'message': f'Error cordoning node {node_name}: {e}'}), 500

@app.route('/cordon_node_only/<node_name>', methods=['POST'])
@login_required
def cordon_node_only(node_name):
    try:
        body = {
            "spec": {
                "unschedulable": True
            }
        }
        core_api.patch_node(name=node_name, body=body)
        return jsonify({'status': 'success', 'message': f'Node {node_name} cordoned successfully.'})
    except Exception as e:
        return jsonify({'status': 'error', 'message': f'Error cordoning node {node_name}: {e}'}), 500

@app.route('/drain_node/<node_name>', methods=['POST'])
@login_required
def drain_node(node_name):
    try:
        # Evict pods from the node
        pods = core_api.list_pod_for_all_namespaces(field_selector=f'spec.nodeName=={node_name}')
        evicted_pods = []
        ignored_pods = []
        errors = []
        for pod in pods.items:
            if pod.metadata.owner_references and any(
                owner.kind == "DaemonSet" for owner in pod.metadata.owner_references
            ):
                ignored_pods.append(f"{pod.metadata.namespace}/{pod.metadata.name}")
                continue

            eviction_body = client.V1Eviction(
                metadata=client.V1ObjectMeta(
                    name=pod.metadata.name,
                    namespace=pod.metadata.namespace
                )
            )
            try:
                core_api.create_namespaced_pod_eviction(
                    name=pod.metadata.name,
                    namespace=pod.metadata.namespace,
                    body=eviction_body
                )
                evicted_pods.append(f"{pod.metadata.namespace}/{pod.metadata.name}")
            except client.ApiException as e:
                errors.append(f"Error evicting pod {pod.metadata.name}: {e.reason}")
        
        if errors:
            return jsonify({'status': 'error', 'message': f'Failed to drain all pods from {node_name}.', 'errors': errors, 'evicted_pods': evicted_pods, 'ignored_pods': ignored_pods}), 500
        return jsonify({'status': 'success', 'message': f'Node {node_name} drained successfully.', 'evicted_pods': evicted_pods, 'ignored_pods': ignored_pods})
    except Exception as e:
        return jsonify({'status': 'error', 'message': f'Error draining node {node_name}: {e}'}), 500

@app.route('/uncordon_node/<node_name>', methods=['POST'])
@login_required
def uncordon_node(node_name):
    try:
        body = {
            "spec": {
                "unschedulable": False
            }
        }
        core_api.patch_node(name=node_name, body=body)
        return jsonify({'status': 'success', 'message': f'Node {node_name} uncordoned successfully.'})
    except Exception as e:
        return jsonify({'status': 'error', 'message': f'Error uncordoning node {node_name}: {e}'}), 500

@app.route('/reboot_node/<node_name>', methods=['POST'])
@login_required
def reboot_node(node_name):
    """Reboots a node using an Ansible playbook."""
    task_id = str(uuid.uuid4())
    playbook_path = os.path.join(os.path.dirname(__file__), 'reboot_node.yml')
    inventory_path = os.path.join(os.path.dirname(__file__), 'inventory.ini')
    
    extra_vars = {'target_host': node_name}
    
    run_ansible_playbook_async(task_id, 'reboot', playbook_path, inventory_path, extra_vars=extra_vars)
    
    return jsonify({'task_id': task_id, 'message': f'Reboot initiated for {node_name}.'}), 202

@app.route('/check_reboot_required/<node_name>')
@login_required
def check_reboot_required(node_name):
    try:
        node = core_api.read_node(name=node_name)
        node_ip = None
        for address in node.status.addresses:
            if address.type == 'InternalIP':
                node_ip = address.address
                break
        
        if not node_ip:
            flash(f"Could not get internal IP for node {node_name}", 'error')
            return redirect(url_for('get_nodes'))

        ssh = paramiko.SSHClient()
        ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())
        ssh.connect(node_ip, username=SSH_USERNAME, key_filename=SSH_KEY_PATH)
        stdin, stdout, stderr = ssh.exec_command('if [ -f /var/run/reboot-required ]; then echo "reboot required"; fi')
        reboot_status = stdout.read().decode('utf-8').strip()
        ssh.close()

        if reboot_status == "reboot required":
            flash(f"Node {node_name} requires a reboot.", 'warning')
        else:
            flash(f"Node {node_name} does not require a reboot.", 'success')
    except Exception as e:
        flash(f"Error checking reboot status for node {node_name}: {e}", 'error')
    return redirect(url_for('get_nodes'))

@app.route('/get_last_reboot_time/<node_name>')
@login_required
def get_last_reboot_time(node_name):
    try:
        node = core_api.read_node(name=node_name)
        node_ip = None
        for address in node.status.addresses:
            if address.type == 'InternalIP':
                node_ip = address.address
                break
        
        if not node_ip:
            flash(f"Could not get internal IP for node {node_name}", 'error')
            return redirect(url_for('get_nodes'))

        ssh = paramiko.SSHClient()
        ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())
        ssh.connect(node_ip, username=SSH_USERNAME, key_filename=SSH_KEY_PATH)
        stdin, stdout, stderr = ssh.exec_command('uptime -s')
        last_reboot_time = stdout.read().decode('utf-8').strip()
        ssh.close()
        flash(f"Last reboot time for node {node_name}: {last_reboot_time}", 'info')
    except Exception as e:
        flash(f"Error getting last reboot time for node {node_name}: {e}", 'error')
    return redirect(url_for('get_nodes'))

@app.route('/nodes')
@login_required
def get_nodes():
    search_query = request.args.get('search')
    try:
        nodes = core_api.list_node(watch=False)
        if search_query:
            nodes.items = [node for node in nodes.items if search_query.lower() in node.metadata.name.lower()]

        nodes_list = []
        for node in nodes.items:
            status_display = "Unknown"
            if node.status and node.status.conditions:
                ready_status = "Unknown"
                for condition in node.status.conditions:
                    if condition.type == "Ready":
                        if condition.status == "True":
                            ready_status = "Ready"
                        else:
                            ready_status = "NotReady"
                        break
                
                if node.spec and node.spec.unschedulable:
                    if ready_status == "Ready":
                        status_display = "Ready,SchedulingDisabled"
                    else:
                        status_display = f"{ready_status},SchedulingDisabled"
                else:
                    status_display = ready_status
            
            roles = [label.split('/')[1] for label, value in node.metadata.labels.items() if 'node-role.kubernetes.io/' in label]

            nodes_list.append({
                'name': node.metadata.name,
                'status': status_display,
                'roles': ", ".join(roles),
                'os_image': node.status.node_info.os_image,
                'unschedulable': node.spec.unschedulable
            })

        return render_template('nodes.html', nodes=nodes_list, search_query=search_query)
    except Exception as e:
        flash(f"Error fetching nodes: {e}", 'error')
        return render_template('nodes.html', nodes=[], search_query=search_query)

@app.route('/get_node_details/<node_name>')
@login_required
def get_node_details(node_name):
    """
    Gets node details using the Kubernetes API and SSH.
    """
    try:
        node = core_api.read_node(name=node_name, pretty=True)
        ready_condition = next((c for c in node.status.conditions if c.type == 'Ready'), None)

        if ready_condition and ready_condition.status == 'True':
            uptime = "N/A"
            reboot_required = False
            patch_status = 'No patches needed'
            try:
                ssh, _ = get_ssh_connection(node_name)
                if ssh == 'local':
                    # Local connection
                    uptime_process = subprocess.run(['uptime', '-p'], capture_output=True, text=True, check=False)
                    if uptime_process.returncode == 0:
                        uptime = uptime_process.stdout.strip().replace('up ', '')
                    reboot_required = os.path.exists('/var/run/reboot-required')
                    sec_process = subprocess.run(['sudo', 'unattended-upgrades', '--dry-run'], capture_output=True, text=True, check=False)
                    security_output = sec_process.stdout + sec_process.stderr
                    if "/usr/bin/dpkg" in security_output and "--unpack" in security_output:
                        patch_status = 'Patches needed'
                else:
                    # Remote connection via SSH
                    try:
                        stdin, stdout, stderr = ssh.exec_command('uptime -p')
                        uptime_output = stdout.read().decode('utf-8').strip()
                        if uptime_output:
                            uptime = uptime_output.replace('up ', '')

                        stdin, stdout, stderr = ssh.exec_command('if [ -f /var/run/reboot-required ]; then echo "reboot required"; fi')
                        reboot_status = stdout.read().decode('utf-8').strip()
                        reboot_required = reboot_status == "reboot required"

                        stdin, stdout, stderr = ssh.exec_command('sudo unattended-upgrades --dry-run')
                        security_output = stdout.read().decode('utf-8') + stderr.read().decode('utf-8')
                        if "/usr/bin/dpkg" in security_output and "--unpack" in security_output:
                            patch_status = 'Patches needed'
                    finally:
                        ssh.close()
            except Exception as e:
                app.logger.error(f"Failed to get details via SSH for {node_name}: {e}")
                pass  # Keep uptime as N/A if SSH fails

            return jsonify({
                'status': 'online',
                'reboot_required': reboot_required,
                'uptime': uptime,
                'patch_status': patch_status
            })
        else:
            return jsonify({'status': 'rebooting', 'uptime': 'N/A', 'patch_status': 'Unknown', 'reboot_required': False})

    except client.ApiException:
        return jsonify({'status': 'rebooting', 'uptime': 'N/A', 'patch_status': 'Unknown', 'reboot_required': False})
    except Exception as e:
        app.logger.error(f"Unexpected error in get_node_details for {node_name}: {e}", exc_info=True)
        return jsonify({'error': 'An unexpected error occurred.', 'uptime': 'Error', 'patch_status': 'Error', 'reboot_required': False})

@app.route('/persistentvolumeclaims')
@login_required
def get_persistentvolumeclaims():
    search_query = request.args.get('search')
    try:
        pvcs = core_api.list_persistent_volume_claim_for_all_namespaces(watch=False)
        if search_query:
            pvcs.items = [pvc for pvc in pvcs.items if search_query.lower() in pvc.metadata.name.lower()]
        return render_template('persistentvolumeclaims.html', pvcs=pvcs.items, search_query=search_query)
    except Exception as e:
        flash(f"Error fetching persistent volume claims: {e}", 'error')
        return render_template('persistentvolumeclaims.html', pvcs=[], search_query=search_query)

@app.route('/delete_pvc/<namespace>/<name>', methods=['POST'])
@login_required
def delete_pvc(namespace, name):
    try:
        core_api.delete_namespaced_persistent_volume_claim(name=name, namespace=namespace)
        flash(f"Persistent Volume Claim {name} in namespace {namespace} deleted successfully.", 'success')
    except Exception as e:
        flash(f"Error deleting Persistent Volume Claim {name} in namespace {namespace}: {e}", 'error')
    return redirect(url_for('get_persistentvolumeclaims'))

@app.route('/configmaps')
@login_required
def get_configmaps():
    search_query = request.args.get('search')
    try:
        configmaps = core_api.list_config_map_for_all_namespaces(watch=False)
        if search_query:
            configmaps.items = [cm for cm in configmaps.items if search_query.lower() in cm.metadata.name.lower()]
        return render_template('configmaps.html', configmaps=configmaps.items, search_query=search_query)
    except Exception as e:
        flash(f"Error fetching configmaps: {e}", 'error')
        return render_template('configmaps.html', configmaps=[], search_query=search_query)

@app.route('/configmap_data/<namespace>/<name>')
@login_required
def configmap_data(namespace, name):
    try:
        configmap = core_api.read_namespaced_config_map(name=name, namespace=namespace)
        return render_template('configmap_data.html', configmap_data=configmap.data, name=name, namespace=namespace)
    except Exception as e:
        return f"Error fetching ConfigMap data: {e}"

@app.route('/statefulsets')
@login_required
def get_statefulsets():
    search_query = request.args.get('search')
    try:
        statefulsets = apps_api.list_stateful_set_for_all_namespaces(watch=False)
        if search_query:
            statefulsets.items = [ss for ss in statefulsets.items if search_query.lower() in ss.metadata.name.lower()]
        return render_template('statefulsets.html', statefulsets=statefulsets.items, search_query=search_query)
    except Exception as e:
        flash(f"Error fetching statefulsets: {e}", 'error')
        return render_template('statefulsets.html', statefulsets=[], search_query=search_query)

@app.route('/rollout_restart_statefulset/<namespace>/<name>', methods=['POST'])
@login_required
def rollout_restart_statefulset(namespace, name):
    try:
        now = datetime.datetime.utcnow().isoformat() + "Z"
        body = {
            "spec": {
                "template": {
                    "metadata": {
                        "annotations": {
                            "kubectl.kubernetes.io/restartedAt": now
                        }
                    }
                }
            }
        }
        apps_api.patch_namespaced_stateful_set(name=name, namespace=namespace, body=body)
        flash(f"StatefulSet {name} in namespace {namespace} restarted successfully.", 'success')
    except Exception as e:
        flash(f"Error restarting StatefulSet {name} in namespace {namespace}: {e}", 'error')
    return redirect(url_for('get_statefulsets'))

@app.route('/delete_statefulset/<namespace>/<name>', methods=['POST'])
@login_required
def delete_statefulset(namespace, name):
    try:
        apps_api.delete_namespaced_stateful_set(name=name, namespace=namespace)
        flash(f"StatefulSet {name} in namespace {namespace} deleted successfully.", 'success')
    except Exception as e:
        flash(f"Error deleting StatefulSet {name} in namespace {namespace}: {e}", 'error')
    return redirect(url_for('get_statefulsets'))

@app.route('/daemonsets')
@login_required
def get_daemonsets():
    search_query = request.args.get('search')
    try:
        daemonsets = apps_api.list_daemon_set_for_all_namespaces(watch=False)
        if search_query:
            daemonsets.items = [ds for ds in daemonsets.items if search_query.lower() in ds.metadata.name.lower()]
        return render_template('daemonsets.html', daemonsets=daemonsets.items, search_query=search_query)
    except Exception as e:
        flash(f"Error fetching daemonsets: {e}", 'error')
        return render_template('daemonsets.html', daemonsets=[], search_query=search_query)

@app.route('/rollout_restart_daemonset/<namespace>/<name>', methods=['POST'])
@login_required
def rollout_restart_daemonset(namespace, name):
    try:
        now = datetime.datetime.utcnow().isoformat() + "Z"
        body = {
            "spec": {
                "template": {
                    "metadata": {
                        "annotations": {
                            "kubectl.kubernetes.io/restartedAt": now
                        }
                    }
                }
            }
        }
        apps_api.patch_namespaced_daemon_set(name=name, namespace=namespace, body=body)
        flash(f"DaemonSet {name} in namespace {namespace} restarted successfully.", 'success')
    except Exception as e:
        flash(f"Error restarting DaemonSet {name} in namespace {namespace}: {e}", 'error')
    return redirect(url_for('get_daemonsets'))

@app.route('/delete_daemonset/<namespace>/<name>', methods=['POST'])
@login_required
def delete_daemonset(namespace, name):
    try:
        apps_api.delete_namespaced_daemon_set(name=name, namespace=namespace)
        flash(f"DaemonSet {name} in namespace {namespace} deleted successfully.", 'success')
    except Exception as e:
        flash(f"Error deleting DaemonSet {name} in namespace {namespace}: {e}", 'error')
    return redirect(url_for('get_daemonsets'))

@app.route('/jobs')
@login_required
def get_jobs():
    search_query = request.args.get('search')
    try:
        jobs = batch_api.list_job_for_all_namespaces(watch=False)
        if search_query:
            jobs.items = [job for job in jobs.items if search_query.lower() in job.metadata.name.lower()]
        return render_template('jobs.html', jobs=jobs.items, search_query=search_query)
    except Exception as e:
        flash(f"Error fetching jobs: {e}", 'error')
        return render_template('jobs.html', jobs=[], search_query=search_query)

@app.route('/secrets')
@login_required
def get_secrets():
    search_query = request.args.get('search')
    try:
        secrets = core_api.list_secret_for_all_namespaces(watch=False)
        if search_query:
            secrets.items = [secret for secret in secrets.items if search_query.lower() in secret.metadata.name.lower()]
        return render_template('secrets.html', secrets=secrets.items, search_query=search_query)
    except Exception as e:
        flash(f"Error fetching secrets: {e}", 'error')
        return render_template('secrets.html', secrets=[], search_query=search_query)

@app.route('/events')
@login_required
def get_events():
    search_query = request.args.get('search')
    try:
        events = core_api.list_event_for_all_namespaces(watch=False)
        if search_query:
            events.items = [event for event in events.items if search_query.lower() in event.metadata.name.lower() or search_query.lower() in event.message.lower()]
        return render_template('events.html', events=events.items, search_query=search_query)
    except Exception as e:
        flash(f"Error fetching events: {e}", 'error')
        return render_template('events.html', events=[], search_query=search_query)

@app.route('/helm_charts')
@login_required
def get_helm_charts():
    search_query = request.args.get('search')
    helm_charts = []
    try:
        # Use subprocess to run the helm command directly
        print("Executing helm list command...")
        list_command = [HELM_EXECUTABLE_PATH, 'list', '--all-namespaces', '-o', 'json', '--kubeconfig', KUBECONFIG_PATH]
        result = subprocess.run(
            list_command,
            capture_output=True,
            text=True,
            check=True
        )
        print(f"Helm list stdout: {result.stdout}")
        print(f"Helm list stderr: {result.stderr}")
        releases = json.loads(result.stdout)
        
        for release in releases:
            print(f"Fetching manifest for chart: {release['name']} in namespace: {release['namespace']}")
            try:
                manifest_command = [HELM_EXECUTABLE_PATH, 'get', 'manifest', release['name'], '--namespace', release['namespace'], '--kubeconfig', KUBECONFIG_PATH]
                manifest_result = subprocess.run(
                    manifest_command,
                    capture_output=True,
                    text=True,
                    check=True
                )
                manifest = manifest_result.stdout
                print(f"Manifest for {release['name']} fetched successfully.")
                
                resources = []
                if manifest.strip(): # Check if manifest is not empty or just whitespace
                    try:
                        for doc in yaml.safe_load_all(manifest):
                            if doc and 'kind' in doc and 'metadata' in doc and 'name' in doc['metadata']:
                                resources.append({
                                    'kind': doc['kind'],
                                    'name': doc['metadata']['name']
                                })
                    except yaml.YAMLError as e:
                        print(f"Error parsing YAML for {release['name']}: {e}")
                        print(f"Problematic manifest content:\n{manifest}")
                        flash(f"Error parsing YAML for {release['name']}: {e}", 'warning')
                else:
                    print(f"Manifest for {release['name']} is empty.")
                    flash(f"Manifest for {release['name']} is empty.", 'info')
                helm_charts.append({
                    'name': release['name'],
                    'namespace': release['namespace'],
                    'chart': release['chart'],
                    'status': release['status'],
                    'resources': resources
                })
            except subprocess.CalledProcessError as e:
                print(f"Error fetching manifest for {release['name']}: {e}")
                print(f"Stderr for manifest command: {e.stderr}")
                flash(f"Error fetching manifest for {release['name']}: {e.stderr}", 'warning')
                # Continue to the next release even if one fails
            except yaml.YAMLError as e:
                print(f"Error parsing YAML for {release['name']}: {e}")
                flash(f"Error parsing YAML for {release['name']}: {e}", 'warning')
                # Continue to the next release even if YAML parsing fails
            except Exception as e:
                print(f"Unexpected error processing manifest for {release['name']}: {e}")
                flash(f"Unexpected error processing manifest for {release['name']}: {e}", 'warning')

        if search_query:
            helm_charts = [chart for chart in helm_charts if search_query.lower() in chart['name'].lower()]

        return render_template('helm_charts.html', helm_charts=helm_charts, search_query=search_query)
    except subprocess.CalledProcessError as e:
        print(f"Error executing helm list: {e}")
        print(f"Stderr from helm list: {e.stderr}")
        flash(f"Error fetching Helm charts: {e.stderr}", 'error')
        return render_template('helm_charts.html', helm_charts=[], search_query=search_query)
    except json.JSONDecodeError as e:
        print(f"Error decoding JSON from helm list output: {e}")
        flash(f"Error decoding Helm list output: {e}", 'error')
        return render_template('helm_charts.html', helm_charts=[], search_query=search_query)
    except Exception as e:
        print(f"General error in get_helm_charts: {e}")
        flash(f"Error fetching Helm charts: {e}", 'error')
        return render_template('helm_charts.html', helm_charts=[], search_query=search_query)


@app.route('/delete_helm_chart/<namespace>/<name>', methods=['POST'])
@login_required
def delete_helm_chart(namespace, name):
    try:
        subprocess.run(
            [HELM_EXECUTABLE_PATH, 'uninstall', name, '--namespace', namespace, '--kubeconfig', KUBECONFIG_PATH],
            capture_output=True,
            text=True,
            check=True
        )
        flash(f"Helm chart {name} in namespace {namespace} deleted successfully.", 'success')
    except Exception as e:
        flash(f"Error deleting helm chart {name} in namespace {namespace}: {e}", 'error')
    return redirect(url_for('get_helm_charts'))

@app.route('/helm_chart_resources/<namespace>/<name>')
@login_required
def get_helm_chart_resources(namespace, name):
    try:
        manifest_result = subprocess.run(
            [HELM_EXECUTABLE_PATH, 'get', 'manifest', name, '--namespace', namespace, '--kubeconfig', KUBECONFIG_PATH],
            capture_output=True,
            text=True,
            check=True
        )
        manifest = manifest_result.stdout
        resources = []
        for doc in yaml.safe_load_all(manifest):
            if doc and 'kind' in doc and 'metadata' in doc and 'name' in doc['metadata']:
                resources.append({
                    'kind': doc['kind'],
                    'name': doc['metadata']['name']
                })
        return render_template('helm_chart_resources.html', chart_name=name, namespace=namespace, resources=resources)
    except Exception as e:
        return f"Error fetching resources for Helm chart {name} in namespace {namespace}: {e}"


@app.route('/compliance_reports')
@login_required
def compliance_reports():
    reports_dir = os.path.join(app.static_folder, 'oscap_reports')
    archive_dir = os.path.join(app.static_folder, 'oscap_reports_archive')
    hosts_list = get_inventory_hosts()
    
    all_reports = []
    
    # Get reports from the main directory
    try:
        for f in os.listdir(reports_dir):
            if f.endswith('.html'):
                all_reports.append(f)
    except FileNotFoundError:
        flash('Compliance reports directory not found.', 'error')

    # Get reports from the archive directory
    try:
        for f in os.listdir(archive_dir):
            if f.endswith('.html'):
                all_reports.append(f)
    except FileNotFoundError:
        # It's okay if the archive directory doesn't exist yet
        pass

    # Sort all reports by modification time, newest first
    def get_mtime(report_name):
        path1 = os.path.join(reports_dir, report_name)
        path2 = os.path.join(archive_dir, report_name)
        if os.path.exists(path1):
            return os.path.getmtime(path1)
        elif os.path.exists(path2):
            return os.path.getmtime(path2)
        return 0

    all_reports.sort(key=get_mtime, reverse=True)
    
    default_report = all_reports[0] if all_reports else None

    return render_template(
        'compliance_reports.html', 
        reports=all_reports, 
        default_report=default_report,
        hosts_list=hosts_list
    )

@app.route('/view_oscap_report/<report_name>')
@login_required
def view_oscap_report(report_name):
    reports_dir = os.path.join(app.static_folder, 'oscap_reports')
    archive_dir = os.path.join(app.static_folder, 'oscap_reports_archive')
    
    report_path = os.path.join(reports_dir, report_name)
    if os.path.exists(report_path):
        return send_from_directory(reports_dir, report_name)
    
    archive_path = os.path.join(archive_dir, report_name)
    if os.path.exists(archive_path):
        return send_from_directory(archive_dir, report_name)
        
    return "Report not found", 404

@app.route('/archived_reports', methods=['GET', 'POST'])
@login_required
def archived_reports():
    reports_dir = os.path.join(app.static_folder, 'oscap_reports')
    archive_dir = os.path.join(app.static_folder, 'oscap_reports_archive')
    search_host = request.form.get('search_host', '')
    search_date = request.form.get('search_date', '')
    hosts_list = get_inventory_hosts()

    all_file_paths = {}
    # Get reports from the archive directory first
    if os.path.exists(archive_dir):
        for f in os.listdir(archive_dir):
            if f.endswith('.html') or f.endswith('.csv'):
                all_file_paths[f] = os.path.join(archive_dir, f)

    # Get reports from the main directory, overwriting archived ones if names conflict
    if os.path.exists(reports_dir):
        for f in os.listdir(reports_dir):
            if f.endswith('.html') or f.endswith('.csv'):
                all_file_paths[f] = os.path.join(reports_dir, f)

    # Filter based on search
    filtered_files = list(all_file_paths.keys())
    if search_host:
        filtered_files = [f for f in filtered_files if search_host.lower() in f.lower()]
    
    if search_date:
        filtered_files = [f for f in filtered_files if search_date in f]

    # Sort by modification time
    filtered_files.sort(key=lambda f: os.path.getmtime(all_file_paths[f]), reverse=True)
    
    return render_template('archived_reports.html', 
                           archived_reports=filtered_files, 
                           hosts_list=hosts_list,
                           search_host=search_host, 
                           search_date=search_date)

@app.route('/view_report/<report_name>')
@login_required
def view_report(report_name):
    reports_dir = os.path.join(app.static_folder, 'oscap_reports')
    archive_dir = os.path.join(app.static_folder, 'oscap_reports_archive')
    
    report_path_main = os.path.join(reports_dir, report_name)
    report_path_archive = os.path.join(archive_dir, report_name)

    if os.path.exists(report_path_main):
        return send_from_directory(reports_dir, report_name)
    elif os.path.exists(report_path_archive):
        return send_from_directory(archive_dir, report_name)
    else:
        return "Report not found", 404

@app.route('/download_selected_reports', methods=['POST'])
@login_required
def download_selected_reports():
    selected_files = request.json.get('files', [])
    if not selected_files:
        return jsonify({'message': 'No files selected.'}), 400

    reports_dir = os.path.join(app.static_folder, 'oscap_reports')
    archive_dir = os.path.join(app.static_folder, 'oscap_reports_archive')
    zip_path = os.path.join('/tmp', f"compliance_reports_{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}.zip")

    with zipfile.ZipFile(zip_path, 'w') as zipf:
        for filename in selected_files:
            path_main = os.path.join(reports_dir, filename)
            path_archive = os.path.join(archive_dir, filename)
            
            file_to_zip = None
            if os.path.exists(path_main):
                file_to_zip = path_main
            elif os.path.exists(path_archive):
                file_to_zip = path_archive
            
            if file_to_zip:
                zipf.write(file_to_zip, filename)

    return send_from_directory('/tmp', os.path.basename(zip_path), as_attachment=True)

@app.route('/delete_selected_reports', methods=['POST'])
@login_required
def delete_selected_reports():
    selected_files = request.json.get('files', [])
    if not selected_files:
        return jsonify({'message': 'No files selected.'}), 400

    reports_dir = os.path.join(app.static_folder, 'oscap_reports')
    archive_dir = os.path.join(app.static_folder, 'oscap_reports_archive')
    deleted_count = 0
    errors = []

    for filename in selected_files:
        path_main = os.path.join(reports_dir, filename)
        path_archive = os.path.join(archive_dir, filename)
        
        file_to_delete = None
        if os.path.exists(path_main):
            file_to_delete = path_main
        elif os.path.exists(path_archive):
            file_to_delete = path_archive

        if file_to_delete:
            try:
                os.remove(file_to_delete)
                deleted_count += 1
            except OSError as e:
                errors.append(f"Error deleting {filename}: {e}")
        else:
            errors.append(f"File not found: {filename}")

    if errors:
        flash(f"Deleted {deleted_count} reports, but encountered errors: {', '.join(errors)}", 'warning')
    else:
        flash(f"Successfully deleted {deleted_count} reports.", 'success')
    
    return jsonify({'message': f'Deleted {deleted_count} reports.'}), 200

@app.route('/get_host_status/<hostname>')
@login_required
def get_host_status(hostname):
    if 'logged_in' not in session:
        return "Unauthorized", 401
    try:
        ssh, ansible_host = get_ssh_connection(hostname)
        os_info = "N/A"
        uptime = "N/A"
        patch_status = 'No patches needed'
        reboot_required = False

        if ssh == 'local':
            # Get OS Info
            os_process = subprocess.run(['lsb_release', '-d'], capture_output=True, text=True, check=False)
            if os_process.returncode == 0:
                os_info = os_process.stdout.split(":")[1].strip()

            # Get Patch Status
            sec_process = subprocess.run(['sudo', 'unattended-upgrades', '--dry-run'], capture_output=True, text=True, check=False)
            security_output = sec_process.stdout + sec_process.stderr
            if "/usr/bin/dpkg" in security_output and "--unpack" in security_output:
                patch_status = 'Patches needed'

            # Get Uptime
            uptime_process = subprocess.run(['uptime', '-p'], capture_output=True, text=True, check=False)
            if uptime_process.returncode == 0:
                uptime = uptime_process.stdout.strip().replace('up ', '')
            
            reboot_required = os.path.exists('/var/run/reboot-required')

        else:
            try:
                # Get OS Info
                stdin, stdout, stderr = ssh.exec_command('lsb_release -d')
                os_output = stdout.read().decode('utf-8')
                if "Description" in os_output:
                    os_info = os_output.split(":")[1].strip()

                # Get Patch Status
                stdin, stdout, stderr = ssh.exec_command('sudo unattended-upgrades --dry-run')
                security_output = stdout.read().decode('utf-8') + stderr.read().decode('utf-8')
                if "/usr/bin/dpkg" in security_output and "--unpack" in security_output:
                    patch_status = 'Patches needed'

                # Get Uptime
                stdin, stdout, stderr = ssh.exec_command('uptime -p')
                uptime = stdout.read().decode('utf-8').strip().replace('up ', '')

                stdin, stdout, stderr = ssh.exec_command('if [ -f /var/run/reboot-required ]; then echo "reboot required"; fi')
                reboot_status = stdout.read().decode('utf-8').strip()
                reboot_required = reboot_status == "reboot required"
            finally:
                ssh.close()

        return jsonify({'status': patch_status, 'os_info': os_info, 'uptime': uptime, 'reboot_required': reboot_required})

    except Exception as e:
        return jsonify({'status': 'Error', 'message': str(e), 'os_info': 'Error', 'uptime': 'Error', 'reboot_required': False})

@app.route('/get_updates/<hostname>/<update_type>')
@login_required
def get_updates(hostname, update_type):
    if 'logged_in' not in session:
        return "Unauthorized", 401
    try:
        ssh, ansible_host = get_ssh_connection(hostname)

        # Get security updates
        if ssh == 'local':
            command = "sudo unattended-upgrades --dry-run -v"
            process = subprocess.run(command.split(), capture_output=True, text=True, check=False)
            security_output = process.stdout + process.stderr
        else:
            stdin, stdout, stderr = ssh.exec_command("sudo unattended-upgrades --dry-run -v")
            security_output = stdout.read().decode('utf-8') + stderr.read().decode('utf-8')

        security_updates_list = []
        for line in security_output.splitlines():
            if line.startswith("Packages that will be upgraded:"):
                packages = line.split(":")[1].strip().split()
                for pkg in packages:
                    if pkg not in security_updates_list:
                        security_updates_list.append(pkg)

        if update_type == 'security':
            return jsonify(security_updates_list)

        # Get all upgradable packages
        if ssh == 'local':
            command = 'apt list --upgradable'
            process = subprocess.run(command.split(), capture_output=True, text=True, check=False)
            upgradable_output = process.stdout + process.stderr
        else:
            stdin, stdout, stderr = ssh.exec_command('apt list --upgradable')
            upgradable_output = stdout.read().decode('utf-8') + stderr.read().decode('utf-8')
            ssh.close()

        non_security_updates = []
        for line in upgradable_output.splitlines():
            if "/" in line and "[" in line and "]" in line:
                try:
                    parts = line.split()
                    package_name_full = parts[0]
                    package_name = package_name_full.split('/')[0]
                    if package_name not in security_updates_list:
                        new_version = parts[1]
                        current_version_match = re.search(r'\[(.*)\]', line)
                        current_version = current_version_match.group(1) if current_version_match else "N/A"
                        non_security_updates.append({
                            'name': package_name,
                            'current_version': current_version,
                            'new_version': new_version
                        })
                except IndexError:
                    print(f"WARNING: Could not parse line: {line}")
                    continue
        
        if update_type == 'other':
            return jsonify(non_security_updates)
            
        return "Invalid update type", 400
    except Exception as e:
        return str(e), 500

@app.route('/run_patch/<hostname>/<patch_type>', methods=['POST'])
@login_required
def run_patch(hostname, patch_type):
    task_id = str(uuid.uuid4())
    
    if patch_type == 'security':
        command_to_run = 'sudo unattended-upgrades --debug'
    elif patch_type == 'non-security':
        command_to_run = 'sudo apt-get update && sudo apt-get dist-upgrade -y'
    else:
        return jsonify({'message': 'Invalid patch type.'}), 400

    def run_patch_async(task_id, hostname, command_to_run, patch_type):
        running_tasks[task_id] = {'status': 'running', 'output': '', 'type': 'patch', 'hostname': hostname, 'patch_type': patch_type}
        try:
            ssh, ansible_host = get_ssh_connection(hostname)
            if ssh == 'local':
                process = subprocess.run(f"sudo {command_to_run}", shell=True, capture_output=True, text=True, check=False)
                output = process.stdout + process.stderr
            else:
                stdin, stdout, stderr = ssh.exec_command(command_to_run)
                output = stdout.read().decode('utf-8') + stderr.read().decode('utf-8')
                ssh.close()
            
            # Save the output to a log file
            log_dir = os.path.join(os.path.dirname(__file__), 'patch_logs')
            if not os.path.exists(log_dir):
                os.makedirs(log_dir)
            
            timestamp = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')
            log_file = os.path.join(log_dir, f'{hostname}_{patch_type}_{timestamp}.log')
            with open(log_file, 'w') as f:
                f.write(output)

            running_tasks[task_id]['output'] = output
            running_tasks[task_id]['status'] = 'completed'
        except Exception as e:
            running_tasks[task_id]['status'] = 'error'
            running_tasks[task_id]['output'] = str(e)

    thread = threading.Thread(target=run_patch_async, args=(task_id, hostname, command_to_run, patch_type))
    thread.start()
    return jsonify({'task_id': task_id, 'message': f'Patching {patch_type} updates for {hostname} started.'}), 202



def get_ssh_connection(hostname):
    inventory_path = os.path.join(os.path.dirname(__file__), 'inventory.ini')
    ansible_host = None
    is_local_connection = False

    with open(inventory_path, 'r') as f:
        for line in f:
            if line.strip().startswith(hostname):
                if 'ansible_connection=local' in line:
                    is_local_connection = True
                    break
                else:
                    match = re.search(r'ansible_host=([0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3})', line)
                    if match:
                        ansible_host = match.group(1)
                        break
    
    if is_local_connection:
        return 'local', None

    if not ansible_host:
        raise Exception(f"Could not find host {hostname} or its IP in inventory.")

    ssh = paramiko.SSHClient()
    ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())
    ssh.connect(ansible_host, username=SSH_USERNAME, key_filename=SSH_KEY_PATH, timeout=10)
    return ssh, ansible_host

@app.route('/upgrade_status')
@login_required
def upgrade_status():
    inventory_path = os.path.join(os.path.dirname(__file__), 'inventory.ini')
    hosts_data = []
    processed_hosts = set()
    kubernetes_nodes = set()

    try:
        # Get Kubernetes nodes
        nodes_obj = core_api.list_node(watch=False)
        kubernetes_nodes = {node.metadata.name for node in nodes_obj.items}

        # Read inventory and process hosts
        with open(inventory_path, 'r') as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith('#') and not line.startswith('['):
                    hostname = line.split(' ')[0]
                    if hostname not in processed_hosts and hostname not in kubernetes_nodes:
                        is_kubegui_controller = 'ansible_connection=local' in line
                        hosts_data.append({
                            'name': hostname,
                            'is_kubegui_controller': is_kubegui_controller,
                            'is_kubernetes_node': False
                        })
                        processed_hosts.add(hostname)
        hosts_data.sort(key=lambda x: x['name'])
    except FileNotFoundError:
        flash("Inventory file not found. Please ensure inventory.ini exists in the application directory.", 'error')
    except Exception as e:
        flash(f"Error processing hosts or Kubernetes nodes: {e}", 'error')

    return render_template('upgrade_status.html', hosts_data=hosts_data)

@app.route('/reboot_selected', methods=['POST'])
@login_required
def reboot_selected():
    selected_hosts = request.json.get('hosts', [])
    if not selected_hosts:
        return jsonify({'message': 'No hosts selected.'}), 400

    inventory_path = os.path.join(os.path.dirname(__file__), 'inventory.ini')
    local_host = None
    try:
        with open(inventory_path, 'r') as f:
            for line in f:
                if 'ansible_connection=local' in line:
                    local_host = line.split()[0]
                    break
    except FileNotFoundError:
        flash('inventory.ini not found!', 'error')
        return jsonify({'message': 'inventory.ini not found!'}), 500

    for host in selected_hosts:
        task_id = str(uuid.uuid4())
        
        if host == local_host:
            playbook_path = os.path.join(os.path.dirname(__file__), 'reboot_localhost.yml')
            extra_vars = None
        else:
            playbook_path = os.path.join(os.path.dirname(__file__), 'reboot_node.yml')
            extra_vars = {'target_host': host}
            
        run_ansible_playbook_async(task_id, 'reboot', playbook_path, inventory_path, extra_vars=extra_vars)
    
    return jsonify({'message': 'Reboot initiated for selected hosts.'}), 202

@app.route('/run_upgrade_check/<hostname>', methods=['POST'])
@login_required
def run_upgrade_check(hostname):
    task_id = str(uuid.uuid4())
    playbook_path = os.path.join(os.path.dirname(__file__), 'ansible_upgrade_check', 'check_upgrades.yml')
    inventory_path = os.path.join(os.path.dirname(__file__), 'inventory.ini')
    
    thread = threading.Thread(target=run_ansible_playbook_async, args=(task_id, 'upgrade_check', playbook_path, inventory_path, None, hostname))
    thread.start()
    return jsonify({'task_id': task_id, 'message': f'Upgrade check started for {hostname}.'}), 202

@app.route('/run_upgrade_check_all', methods=['POST'])
@login_required
def run_upgrade_check_all():
    task_id = str(uuid.uuid4())
    playbook_path = os.path.join(os.path.dirname(__file__), 'ansible_upgrade_check', 'check_upgrades.yml')
    inventory_path = os.path.join(os.path.dirname(__file__), 'inventory.ini')
    
    thread = threading.Thread(target=run_ansible_playbook_async, args=(task_id, 'upgrade_check_all', playbook_path, inventory_path))
    thread.start()
    return jsonify({'task_id': task_id, 'message': 'Upgrade check started for all hosts.'}), 202

@app.route('/run_unattended_upgrades/<hostname>', methods=['POST'])
@login_required
def run_unattended_upgrades(hostname):
    task_id = str(uuid.uuid4())
    playbook_path = os.path.join(os.path.dirname(__file__), 'ansible_upgrade_check', 'run_unattended_upgrades.yml')
    inventory_path = os.path.join(os.path.dirname(__file__), 'inventory.ini')
    
    thread = threading.Thread(target=run_ansible_playbook_async, args=(task_id, 'unattended_upgrades', playbook_path, inventory_path, None, hostname))
    thread.start()
    return jsonify({'task_id': task_id, 'message': f'Unattended upgrades started for {hostname}.'}), 202

@app.route('/get_all_updates/<hostname>')
@login_required
def get_all_updates(hostname):
    if 'logged_in' not in session:
        return "Unauthorized", 401
    try:
        ssh, ansible_host = get_ssh_connection(hostname)
        if ssh == 'local':
            command = 'apt list --upgradable'
            process = subprocess.run(command.split(), capture_output=True, text=True, check=False)
            output = process.stdout + process.stderr
        else:
            stdin, stdout, stderr = ssh.exec_command('apt list --upgradable')
            output = stdout.read().decode('utf-8') + stderr.read().decode('utf-8')
            ssh.close()
        return Response(output, mimetype='text/plain')
    except Exception as e:
        return str(e), 500

@app.route('/get_non_security_updates/<hostname>')
@login_required
def get_non_security_updates(hostname):
    if 'logged_in' not in session:
        return "Unauthorized", 401
    try:
        ssh, ansible_host = get_ssh_connection(hostname)
        
        # Get security updates first to filter them out
        if ssh == 'local':
            command = 'sudo unattended-upgrades --dry-run'
            process = subprocess.run(command.split(), capture_output=True, text=True, check=False)
            security_output = process.stdout + process.stderr
        else:
            stdin, stdout, stderr = ssh.exec_command('sudo unattended-upgrades --dry-run')
            security_output = stdout.read().decode('utf-8') + stderr.read().decode('utf-8')

        security_updates_list = []
        for line in security_output.splitlines():
            if "/usr/bin/dpkg" in line:
                match = re.search(r'/var/cache/apt/archives/(.+?)_', line)
                if match:
                    package_name = match.group(1)
                    if package_name not in security_updates_list:
                        security_updates_list.append(package_name)

        # Get all upgradable packages
        if ssh == 'local':
            command = 'apt list --upgradable'
            process = subprocess.run(command.split(), capture_output=True, text=True, check=False)
            upgradable_output = process.stdout + process.stderr
        else:
            stdin, stdout, stderr = ssh.exec_command('apt list --upgradable')
            upgradable_output = stdout.read().decode('utf-8') + stderr.read().decode('utf-8')
            ssh.close()

        non_security_updates = []
        for line in upgradable_output.splitlines():
            if "/" in line and "[" in line and "]" in line:
                try:
                    parts = line.split()
                    package_name_full = parts[0]
                    package_name = package_name_full.split('/')[0]
                    if package_name not in security_updates_list:
                        new_version = parts[1]
                        current_version_match = re.search(r'\[(.*)\]', line)
                        current_version = current_version_match.group(1) if current_version_match else "N/A"
                        non_security_updates.append(f"{package_name} (Current: {current_version}, New: {new_version})")
                except IndexError:
                    print(f"WARNING: Could not parse line: {line}")
                    continue
        
        return Response('\n'.join(non_security_updates), mimetype='text/plain')
    except Exception as e:
        return str(e), 500

@app.route('/non_security_updates_output/<hostname>')
@login_required
def non_security_updates_output(hostname):
    # This route is kept for backward compatibility if needed, 
    # but get_non_security_updates is more direct.
    return "This is a placeholder. Use /get_non_security_updates/<hostname> instead."

@app.route('/download_summary_csv')
@login_required
def download_summary_csv():
    reports_dir = os.path.join(app.static_folder, 'oscap_reports')
    archive_dir = os.path.join(app.static_folder, 'oscap_reports_archive')
    
    latest_csv = None
    latest_mtime = 0
    
    all_dirs = [reports_dir]
    if os.path.exists(archive_dir):
        all_dirs.append(archive_dir)

    for directory in all_dirs:
        if os.path.exists(directory):
            for f in os.listdir(directory):
                if f.endswith('.csv') and 'oscap_detailed_summary' in f:
                    path = os.path.join(directory, f)
                    mtime = os.path.getmtime(path)
                    if mtime > latest_mtime:
                        latest_csv = path
                        latest_mtime = mtime

    if latest_csv:
        return send_from_directory(os.path.dirname(latest_csv), os.path.basename(latest_csv), as_attachment=True)
    else:
        flash('No detailed summary CSV found. Please run an OpenSCAP scan first.', 'warning')
        return redirect(url_for('compliance_reports'))

@app.route('/download_report/<report_name>/<file_type>')
@login_required
def download_report(report_name, file_type):
    reports_dir = os.path.join(app.static_folder, 'oscap_reports')
    archive_dir = os.path.join(app.static_folder, 'oscap_reports_archive')
    
    report_path = os.path.join(reports_dir, report_name)
    if not os.path.exists(report_path):
        report_path = os.path.join(archive_dir, report_name)

    if not os.path.exists(report_path):
        flash('Report not found.', 'error')
        return redirect(url_for('compliance_reports'))

    if file_type == 'pdf':
        try:
            # Check if wkhtmltopdf is installed
            try:
                subprocess.run(['which', 'wkhtmltopdf'], check=True, capture_output=True)
            except (subprocess.CalledProcessError, FileNotFoundError):
                flash('Error: wkhtmltopdf is not installed. Please install it to generate PDF reports.', 'error')
                return redirect(url_for('compliance_reports'))

            pdf_path = os.path.join('/tmp', report_name.replace('.html', '.pdf'))
            pdfkit.from_file(report_path, pdf_path)
            return send_from_directory('/tmp', report_name.replace('.html', '.pdf'), as_attachment=True)
        except Exception as e:
            flash(f"Error converting to PDF: {e}", 'error')
            return redirect(url_for('compliance_reports'))
    else:
        # For downloading the HTML file itself
        if os.path.dirname(report_path) == reports_dir:
            return send_from_directory(reports_dir, report_name, as_attachment=True)
        else:
            return send_from_directory(archive_dir, report_name, as_attachment=True)



@app.route('/run_patch_selected/<patch_type>', methods=['POST'])
@login_required
def run_patch_selected(patch_type):
    selected_hosts = request.json.get('hosts', [])
    if not selected_hosts:
        return jsonify({'message': 'No hosts selected.'}), 400

    for host in selected_hosts:
        task_id = str(uuid.uuid4())
        if patch_type == 'security':
            command_to_run = 'sudo unattended-upgrades --debug'
        elif patch_type == 'non-security':
            command_to_run = 'sudo apt-get update && sudo apt-get dist-upgrade -y'
        else:
            return jsonify({'message': 'Invalid patch type.'}), 400

        def run_patch_async(task_id, hostname, command_to_run, patch_type):
            running_tasks[task_id] = {'status': 'running', 'output': '', 'type': 'patch', 'hostname': hostname, 'patch_type': patch_type}
            try:
                ssh, ansible_host = get_ssh_connection(hostname)
                if ssh == 'local':
                    process = subprocess.run(command_to_run, shell=True, capture_output=True, text=True, check=False)
                    output = process.stdout + process.stderr
                else:
                    if patch_type == 'non-security':
                        stdin, stdout, stderr = ssh.exec_command('sudo apt-get update')
                        output1 = stdout.read().decode('utf-8') + stderr.read().decode('utf-8')
                        
                        stdin, stdout, stderr = ssh.exec_command('sudo apt-get dist-upgrade -y')
                        output2 = stdout.read().decode('utf-8') + stderr.read().decode('utf-8')
                        output = output1 + "\n--- apt-get dist-upgrade ---\n" + output2
                    else:
                        stdin, stdout, stderr = ssh.exec_command(command_to_run)
                        output = stdout.read().decode('utf-8') + stderr.read().decode('utf-8')
                    ssh.close()
                
                log_dir = os.path.join(os.path.dirname(__file__), 'patch_logs')
                if not os.path.exists(log_dir):
                    os.makedirs(log_dir)
                
                timestamp = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')
                log_file = os.path.join(log_dir, f'{hostname}_{patch_type}_{timestamp}.log')
                with open(log_file, 'w') as f:
                    f.write(output)

                if 'No packages found that can be upgraded' in output:
                    running_tasks[task_id]['status'] = 'completed'
                else:
                    running_tasks[task_id]['status'] = 'error'
                    running_tasks[task_id]['output'] = 'unattended-upgrades command failed'
            except Exception as e:
                running_tasks[task_id]['status'] = 'error'
                running_tasks[task_id]['output'] = str(e)

        thread = threading.Thread(target=run_patch_async, args=(task_id, host, command_to_run, patch_type))
        thread.start()
    
    return jsonify({'message': f'Patching {patch_type} updates for selected hosts started.'}), 202


@app.route('/ollama_models')
@login_required
def ollama_models():
    try:
        response_data = ollama.list()
        models_list = response_data.get('models', [])
        
        serializable_models = [
            {
                'name': model.get('name'),
                'model': model.get('model'),
                'modified_at': str(model.get('modified_at')),
                'size': model.get('size')
            } 
            for model in models_list
        ]
        
        return jsonify(models=serializable_models)
    except Exception as e:
        print(f"Error in /ollama_models endpoint: {e}")
        return jsonify(error=str(e)), 500

@app.route('/ollama_models')
@login_required
def ollama_models():
    try:
        response_data = ollama.list()
        models_list = response_data.get('models', [])
        
        serializable_models = [
            {
                'name': model.get('name'),
                'model': model.get('model'),
                'modified_at': str(model.get('modified_at')),
                'size': model.get('size')
            } 
            for model in models_list
        ]
        
        return jsonify(models=serializable_models)
    except Exception as e:
        print(f"Error in /ollama_models endpoint: {e}")
        return jsonify(error=str(e)), 500

client = ollama.Client(host='http://127.0.0.1:11434')

@app.route('/chatbot', methods=['GET', 'POST'])
@login_required
def chatbot_route():
    if request.method == 'POST':
        user_message = request.json['message']
        model = request.json.get('model', 'tinyllama')
        try:
            response = client.chat(
                model=model,
                messages=[
                    {'role': 'user', 'content': user_message}
                ]
            )
            message_content = response['message']['content']
            return jsonify({'response': message_content})
        except Exception as e:
            print(f"Error during Ollama chat: {e}")
            return jsonify({'response': 'Error processing chatbot response.'}), 500
    return render_template('chatbot.html')

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5001)