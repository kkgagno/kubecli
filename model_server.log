--- Initializing Model Server ---
--- Loading model: meta-llama/Meta-Llama-3-8B-Instruct ---
--- Using cache directory: /home/keith/.cache/huggingface ---
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 174.27it/s]
Some parameters are on the meta device because they were offloaded to the disk and cpu.
Device set to use cpu
--- Model loaded successfully. ---
--- Starting model server on port 5002 ---
 * Serving Flask app 'model_server'
 * Debug mode: off
[31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:5002
 * Running on http://192.168.122.182:5002
[33mPress CTRL+C to quit[0m
